---
layout: ar-post
lang: ar
title:  "مُلخص خوارزميّة المُحوِّل"
author: mohamed-ali
image_thumbnail: assets/images/tramsformer-ar-detailed.png
categories: [ التعلّم الآلي , العربية , التعلّم العميق , المُحوّل ]
featured: true
hidden: false
published: true

---

خوازمية المُحوّل هي إحدي الخوازميات الأكثر فعالية ورواجا في هذه الفترة. وقد صيغ هذا الإسم ونُشر تصميم هذه الخوارزمية بادِئ الأمر سنة 2017 في الورقة البحثية المشهورة المُسمات "الإنتباه هو كُلُّ ما تحتاجه"[1].
فماهي طريقة عملها ومالذي يُميّزها؟

صُممت خوازمية المُحوّل لحلّ مشاكل التعلُّم على التسلسُلات. وهي تندرج في صنف التعلُّم من التسلسُل إلى التسلسُل، ونقصد بذلك النماذج وطُرق التعلُّم التي تُمكّن من تعليم نموذج ما كيف يُحوِّلُ سلسلةً مُدخلة ما إلى سلسلةٍ أخرى مُقابلة لها.
مثلا، قد تكون السلسلة المُدخلة هي جُملةٌ باللغة الإنجليزية والسلسلة المُخرجة هي ترجمةُ تلك الجُملة إلى اللُغة العربية. أو قد تكون السلسلة المُدخلة مقطعًا صوتيا والسلسلة المُخرجة هي الجُملة النصيّة المُوافقة لما في المقطع الصوتي.
وفي العموم، المقصود بالتسلسُل: أيُّ بيانات مُدخلة تتكون من رموز متوالية تؤدّي معنى معًا، ومنها الجُمل والأصوات والفيديوهات ... إلخ. 

 كغيرها من خوارزميات التعلُّم على التسلسُلات التي انتشرت منذ سنة 2014، صُممت خوازمية المُحوِّل اعتمادا على هيكلة تتكوّن من جُزئين: الترميز + فكّ الترميز.
وقُدِّم هذا التصميم في الورقة البحثية "التعلُّم من التسلسُل إلى التسلسُل باستعمال الشبكات العصبية"[2] حيثُ أدَّى إلى تحسينِ أداء التعلُّم الآلي بشكل ملحوظ على مهمّة الترجمة، ممّا أدّى إلي إنتشار إستعماله في مجتمع البحث العلمي.

<img class="img-fluid" src="/assets/images/tramsformer-overview-ar.png" alt="نظرة عامة في هيكلة خوارزمية المُحوِّل">

## هيكلة الترميز وفكّ الترميز في خوارزمية المُحوّل 

حسب [1] يتكوّن كُلٌّ من شطر الترميز وفكّ الترميز من 6 طبقات، و6 هو اختيار المؤلفين ولك الحرية في تغييره مع الاخذ بعين الاعتبار أنّ زيادة عدد الطبقات تؤدّي إلى زيادة عدد العمليات الحسابية في النموذج،
ممّا يؤثّر في سُرعةِ وتكلفة عمليّتَي التدريب والاستعمال.
<img class="img-fluid" src="/assets/images/tramsformer-ar-details.png" alt="نظرة عامة في هيكلة خوارزمية المُحوِّل مع توضيح الوصلات بين الطبقات الستة">

في شطر الترميز، تتكوّن كُلُّ طبقة من الطبقات الستة من طبقتَين فرعيتَين. الطبقة الفرعية الأولي هي طبقة الإنتباه الذاتي مُتعدّد الرؤوس. والطبقة الفرعية الثانية هي شبكة عصبية مُتصّلة بالكامل عادية.
يُضاف مُدخل الطبقة الفرعية إلى مُخرجها وذلك ما يُسمّى بتقنية "الوصلات المتبقية"[3] والتي تُسهّل عمليّة التحسين لإيجاد أفضل قيم لمُعاملات النموذج.
ثم تُطبّق عمليّة "التسوية"[4] على نتيجة جمع المُدخل مع المُخرج.    

أمّا في شطر فكّ الترميز، تتكوّن كُلُّ طبقة من الطبقات الستة من ثلاث طبقات مُتوالية. الطبقتان الفرعيتان الأولتان هُما طبقتا إنتباه ذاتي مُتعدّد الرؤوس. والطبقة الفرعية الثالثة هي شبكة عصبية مُتصّلة بالكامل عادية.
في هذا الشطر أيضا، يُضاف مُدخل كُل طبقة إلى مُخرجها ثم تُطبّق عمليّة "التسوية"[4] على نتيجة جمع المُدخل مع المُخرج.    

<img class="img-fluid" src="/assets/images/tramsformer-detailed-ar.png" alt="نظرة تفصيلية في تركيبة معماريّة خوارزمية المُحوِّل">

### الإنتباه الذاتي مُتعدّد الرؤوس

الفرق بين خوارزمية المُحوِّل ونظائرها من الخوازميات العاملة على تسلسُلات البيانات هي إعتمادُها أساسا على آلية الإنتباه.
فكما تبيّن في الرسم التفصيلي السّابق، تحتوي طبقة الترميز على طبقة إنتباه مُتعدّد الرؤوس بينما تحتوي طبقة فكِّ الترميز على طبقتَي إنتباه.
ولذا فإن فهم ما تقوم به خوازمية المُحوّل يحتاج إلى تفصيل لألية الإنتباه المُستعملة بكثرة داخله.

نرى في الرسم التفصيلي التّالي على اليمين تركيبية طبقة الإنتباه الذاتي مُتعدّد الرؤوس، حيثُ يتبين أنّها ترتكز على آلية الإنتباه المُعتمد على الجداء السُلَّمِي المُحجَّم. وهذه الآلية ماهي إلاّ سلسلة من العملّيات الرياضية المُبيّنة في الرسم التفصيلي على اليسار.    

<img class="img-fluid" src="/assets/images/attention-mechanism-details.png" alt="نظرة تفصيلية في الانتباه مُتعدّد الرؤوس">

### معجم الألفاظ التقنية والمُرادفات الإنجليزية

- المُحوّل: transformer
- الترميز: encoding
- فكّ الترميز: decoder
- المُرمِّز: encoder
- فاكُّ الترميز: decoder
- دالّة الحدّ الأقصى الليّن: softmax
- طبقة تحويل خطّي: linear transformation layer
- الإنتباه: attention
- الإنتباه مُتعدد الرؤوس: multi-head attention
- التضمين: embedding
- الترميز الموضعي: positional encoding
- التسوية: normalisation
- الإنتباه المُعتمد على الجداء السُلَّمِي المُحجَّم: Scaled Dot-Product Attention

### المصادر
	
1. [الإنتباه هو كُلُّ ما تحتاجه](https://arxiv.org/abs/1706.03762)
2. [التعلُّم من التسلسُل إلى التسلسُل باستعمال الشبكات العصبية](https://arxiv.org/abs/1409.3215)
3. [الوصلات المُتبقية](https://paperswithcode.com/method/residual-connection)
4. [التسوية](https://paperswithcode.com/method/batch-normalization) 
