---
layout: ar-post
lang: ar
title:  "مُلخص خوارزميّة المُحوِّل"
author: mohamed-ali
image: assets/images/16.jpg
categories: [ التعلّم الآلي , العربية , التعلّم العميق , المُحوّل ]
featured: true
hidden: false
published: true

---

خوازمية المُحوّل هي إحدي الخوازميات الأكثر فعالية ورواجا في هذه الفترة. وقد صيغ هذا الإسم ونُشر تصميم هذه الخوارزمية بادِئ الأمر في الورقة البحثية المشهورة المُسمات "الإنتباه هو كُلُّ ما تحتاجه"[1] التي نُشرت سنة 2017.
فماهي طريقة عملها ومالذي يُميّزها؟

صُممت خوازمية المُحوّل لحلّ مشاكل التعلُّم على التسلسُلات. وهي تندرج في صنف التعلُّم من التسلسُل إلى التسلسُل، ونقصد بذلك النماذج وطُرق التعلُّم التي تُمكّن من تعليم نموذج ما كيف يُحوِّلُ سلسلةً مُدخلة ما إلى سلسلةٍ أخرى مُقابلة لها.
مثالا، قد تكون السلسلة المُدخلة هي جُملةٌ باللغة الإنجليزية والسلسلة المُخرجة هي ترجمةُ تلك الجُملة إلى اللُغة العربية. أو قد تكون السلسلة المُدخلة مقطعًا صوتيا والسلسلة المُخرجة هي الجُملة النصيّة المُوافقة لما في المقطع الصوتي.
وفي العموم، المقصود بالتسلسُل: أيُّ بيانات مُدخلة تتكون من رموز متوالية تؤدّي معنى معًا، ومنها الجُمل والأصوات والفيديوهات ... إلخ. 

 كغيرها من خوارزميات التعلُّم على التسلسُلات التي انتشرت منذ سنة 2014، صُممت خوازمية المُحوِّل اعتمادا على هيكلة تتكوّن من جُزئين: الترميز + فكّ الترميز.
وقُدِّم هذا التصميم في الورقة البحثية "التعلُّم من التسلسُل إلى التسلسُل باستعمال الشبكات العصبية"[2] حيثُ أدَّى إلى تحسينِ أداء التعلُّم الآلي بشكل ملحوظ على مهمّة الترجمة، ممّا أدّى إلي إنتشار إستعماله في مجتمع البحث العلمي.

<img class="img-fluid" src="/assets/images/tramsformer-encoder-decoder-overview.png" alt="نظرة عامة في هيكلة خوارزمية المُحوِّل">

## الترميز:

حسب [1] يتكوّن جزء الترميز من 6 طبقات، و6 هو اختيار المؤلفين ولك الحرية في تغييره مع الاخذ بعين الاعتبار أنّ زيادة عدد الطبقات تؤدّي إلى زيادة عدد العمليات الحسابية اللازمة لتدريب النموذج،
ممّا يؤثّر على سُرعةِ وتكلفة عمليّة التدريب.
<img class="img-fluid" src="/assets/images/tramsformer-ar-detailed.png" alt="نظرة عامة في هيكلة خوارزمية المُحوِّل مع توضيح وصلات بين الطبقات الستة">

كُلُّ طبقة من الطبقات الستة تتكوّن بدورها من طبقتَين فرعيتَين. الطبقة الفرعية الأولي هي طبقة الإنتباه الذاتي مُتعدّد الرؤوس. والطبقة الفرعية الثانية هي شبكة عصبية مُتصّلة بالكامل عادية.
يُضاف مُدخل الطبقة الفرعية إلى مُخرجها وذلك ما يُمسمّى بتقنية "الوصلات المتبقية"[3] والتي تُسهّل عمليّة التحسين لإجاد أفضل قيم لمُعاملات النموذج.
ثم تُطبّق عمليّة "التسوية"[4] على نتيجة جمع المُدخل مع المُخرج.    

### الإنتباه الذاتي مُتعدّد الرؤوس

المصادر:
	
1. [الإنتباه هو كُلُّ ما تحتاجه](https://arxiv.org/abs/1706.03762)
2. [التعلُّم من التسلسُل إلى التسلسُل باستعمال الشبكات العصبية](https://arxiv.org/abs/1409.3215)
3. [الوصلات المُتبقية](https://paperswithcode.com/method/residual-connection)
4. [التسوية](https://paperswithcode.com/method/batch-normalization) 
