---
layout: ar-post
lang: ar
title:  "مُلخص خوارزميّة المُحوِّل"
author: mohamed-ali
image_thumbnail: assets/images/tramsformer-ar-detailed.png
categories: [ التعلّم الآلي , العربية , التعلّم العميق , المُحوّل ]
featured: true
hidden: false
published: true

---

خوارزمية المُحوّل هي إحدي الخوارزميات الأكثر فعالية ورواجا في هذه الفترة. وقد صيغ هذا الاسم ونُشر تصميمُ هذه الخوارزمية بادِئ الأمر سنة 2017 في الورقة البحثيّة المشهورة "الانتباه هو كُلُّ ما تحتاجه"[1].
فماهي طريقة عملها ومالذي يُميّزها؟

صُممت خوارزمية المُحوّل لحلّ مشاكل التعلُّم على التسلسُلات. وهي تندرج في صنف التعلُّم من التسلسُل إلى التسلسُل، ونقصد بذلك النماذج وطُرق التعلُّم التي تُمكِّن من تعليم نموذجٍ ما كيفَ يُحوِّلُ سلسلةً مُدخلة ما إلى سلسلةٍ أخرى مُقابِلة لها.
مثلا، قد تكونُ السلسلة المُدخلة هي جُملةٌ باللغة الإنجليزية والسلسلةُ المُخرجة هي ترجمةُ تلك الجُملة إلى اللُغة العربية. أو قد تكون السلسلة المُدخلة مقطعًا صوتيا والسلسلة المُخرجة هي الجُملة النصيّة المُوافقة لما في المقطع الصوتي.
وفي العموم، المقصود بالتسلسُل: أيُّ بياناتٍ مُدخلة تتكون من رموز متوالية تؤدّي معنى معًا، ومنها الجُمل والأصوات والفيديوهات ... إلخ. 

 كغيرها من خوارزميات التعلُّم على التسلسُلات التي انتشرت منذ سنة 2014، صُممت خوارزمية المُحوِّل اعتمادً على هيكلة تتكوّن من جُزئين: الترميز + فكّ الترميز.
وقُدِّم هذا التصميم في الورقة البحثيّة "التعلُّم من التسلسُل إلى التسلسُل باستعمال الشبكات العصبية"[2] حيثُ أدَّى إلى تحسينِ أداء التعلُّم الآلي بشكل ملحوظ على مهمَّة الترجمة، ممّا أدّى إلي انتشار استعماله في مجتمع البحث العلمي.

<img class="img-fluid" src="/assets/images/tramsformer-overview-ar.png" alt="نظرة عامة في هيكلة خوارزمية المُحوِّل">

## هيكلة الترميز وفكّ الترميز في خوارزمية المُحوّل 

حسب [1] يتكوّن كُلٌّ من شطر الترميز وفكّ الترميز من 6 طبقات، و6 هو اختيار المؤلفين ولك الحرية في تغييره مع الاخذ بعين الاعتبار أنّ زيادة عدد الطبقات تؤدّي إلى زيادة عدد العمليّات الحسابية في النموذج،
ممّا يؤثِّر في سُرعةِ وتكلُفة عملِيّتَي التدريب والاستعمال.
<img class="img-fluid" src="/assets/images/tramsformer-ar-details.png" alt="نظرة عامة في هيكلة خوارزمية المُحوِّل مع توضيح الوصلات بين الطبقات الستة">

في شطر الترميز، تتكوَّن كُلُّ طبقة من الطبقات الستة من طبقتَين فرعيتَين. الطبقة الفرعية الأولي هي طبقةُ الإنتباه الذاتي مُتعدّد الرؤوس. والطبقة الفرعية الثانية هي شبكة تغذية أمامية، وهي عبارة عن شبكة عصبية مُتصّلة بالكامل عادية.
يُضاف مُدخل كُلِّ طبقة فرعية إلى مُخرجها وذلك ما يُسمَّى بتقنية "وصلة البقية"[3] والتي تُسهِّل عمليّة التحسين لإيجاد أفضلِ قيمٍ لمُعاملات النموذج.
ثم تُطبَّق عمليّة "التسوية"[4] على نتيجة جمع المُدخل مع المُخرج. أنظر في الشطر الأيمن للرسم التفصيلي التّالي لتتوضّح البِنية.

أمّا في شطر فكّ الترميز، تتكوّن كُلُّ طبقة من الطبقات الستة من ثلاث طبقات مُتوالية. الطبقتان الفرعيتان الأولتان هُما طبقتا انتباه ذاتي مُتعدّد الرؤوس. والطبقة الفرعية الثالثة هي طبقة تغذية أماميّة.
في هذا الشطر أيضا، يُضاف مُدخل كُل طبقة إلى مُخرجها ثم تُطبّق عمليّة "التسوية"[4] على نتيجة جمع المُدخل مع المُخرج. أنظر في الشطر الأيسر للرسم التفصيلي التّالي لتتوضّح البنية.

<img class="img-fluid" src="/assets/images/tramsformer-detailed-ar.png" alt="نظرة تفصيلية في تركيبة معماريّة خوارزمية المُحوِّل">

### الاِنتباه الذاتي مُتعدّد الرؤوس

الفرق بين خوارزمية المُحوِّل ونظائرها من الخوارزمات العاملة على تسلسُلات البيانات هي اِعتمادُها أساسا على آلية الانتباه.
فكما تبيّن في الرسم التفصيلي السّابق، تحتوي طبقة الترميز على طبقة اِنتباه مُتعدِّدِ الرؤوس بينما تحتوي طبقة فكِّ الترميز على طبقتَي اِنتباه.
ولذا فإن فهم ما تقوم به خوارزمية المُحوّل يحتاج إلى تفصيل لآليّة الانتباه المُستعملة بكثرة داخله.

نرى في الرسم التفصيلي التّالي على اليمين تركيبية طبقة الانتباه الذاتي مُتعدّد الرؤوس، حيثُ يتبين أنّها ترتكز على آلية الانتباه المُعتمد على الجداء السُلَّمِي المُحجَّم. وهذه الآلية ماهي إلاّ سلسلةٌ من العمليّات الرياضيّة المُبيَّنة في الرسم التفصيلي على اليسار.    

<img class="img-fluid" src="/assets/images/attention-mechanism-details.png" alt="نظرة تفصيلية في الانتباه مُتعدّد الرؤوس">

### فهم آليّة الانتباه

حتّى الآن، تعمّقنا في تفاصيل مِعماريّة المُحوِّل حتّى تبيّن الدَّور المركزي لآلية الاِنتباه فيها. لابُدّ الآن من الوقوف على هذه الآلية بعُمقٍ أكبر لفهم تفاصيل عملها وأسباب كفاءتها. فلنبدأ أوّلا باستذكار المُشكل الأساسي الذي صُممت آلية الاِنتباه لحلّه. 

#### ما المُشكل الذي صُمّمت آليّة الاِنتباه لحلّه؟

الإجابة عن هذا السؤال ستُساعد في فهم الأسباب التّي أدّت إلى تصميم آلية الاِنتباه بشكلها الحالي، ولذا فهمها بشكل أعمق. ولتحديد هذا المُشكل علينا العودة بالزمن إلى الفترة قبل اِكتشاف قُدرة آلية الاِنتباه على حلّ مشاكل التعلّم على التسلسلات. حينها، كانت الخوارزمية الأكثر كفاءة في التعلّم على التسلسُلات هي الشبكة العصبيّة التكراريّة والتي تشتهر باسمها المُختصر الRNN، وفيها نوعان هُما LSTM و GRU. يُمكِنك البحث عن تفاصيل نَوعَي الRNN لاحقا، فلن نخوض فيهما هنا لأنّ عدم معرفتهما لا تمنع من فهم تصميم المُحوّل و الانتباه بدقّة.

سنكتفي بنظرةٍ عامّة موجزة على الشبكة العصبية التكراريّة حتّى نفهم نقطة ضعفها التّي دفعت إلى اِكتشاف آلية الاِنتباه. لنعتبر أنّ لدينا تسلسُلا مُدخلا، يتكوَّن من عدَّة عناصر، مثلا جُملةٌ تتكوَّنُ من عِدَّة كلمات. لو اعتبرنا عدد العناصر $$m$$ و رمزنا ب $$x$$ لكُلِّ عُنصر، يُمكننا أن نرمُز للتسلسل ككُل هكذا: 
$$ x_m, ...,x_1 $$
.

في الرسم التوضيحي التّالي، تجد بسطًا للمُعالجة التكرارية الحاصلة في خليّة الشبكة العصبيّة التكراريّة، حيثُ تُظهر الروابط باللّون الأحمر أنّه للقيام بحساب الخليّة في الوقت t يجب توفير قِيَم المُدخل x في الوقت الحالي $$x_t $$ بالإضافة إلى قيم مُخرج هذه الخلية في الوقت السّابق t - 1. و بما أنّ حساب الخليّة في الوقت السابق t - 1 يحتاج إلى حساب الوقت الذي قبله، نفهم أنّنا نحتاج إلى القيام بالحسابات على كُلّ عناصر تسلسل البيانات السابقة، واحدة تلو الأخرى عبر نفس الخليّة، حتّى نتمكّن من حساب العنصر في الوقت الحالي t. ممّا يعني عدم إمكانيّة حساب كُلّ عناصر المُخرجات  $$y_t$$  بالتّوازي، لأنّ حساب كُلِّ مُخرج يعتمِدُ على حسابِ المُخرجِ الذِّي قبله.  

<img class="img-fluid" src="/assets/images/RNN_overview.png" alt="نظرة عامّة على ال RNN">

لذا فنقطة الضعف الأساسية في الشبكة العصبيّة التكراريّة هي ناتجة عن هذه الوصلات التكراريّة في تصميمها. فرغم أنّها تُساعد في تمرير المعلومات على طول التسلسُل، إلاّ أنّ وجودها يعني عدم إمكانية استخراج الحسابات في الوقت t من دون القيام بالحسابات في الأوقاتِ السّابقة. وهو ما يحدُّ من سرعة التعلُّم في هذه الخوارزمية، فيؤدِّي إلى بُطئ عمليّة تدريب نماذج الشبكة العصبية التكرارية ككُل.

**لذا فهدفُ التصميم الذّي اِنطُلِقَ منه لبلوغ آلية الاِنتباه هُوَ: اِيجادُ طريقةٍ للتعلُّمِ من سلاسلِ البياناتِ 
بشكلٍ مُتوازي، وليس تسلسُلي أو تكراري، أي اِيجادُ بديلٍ للتعلُّم من دون الوصلات التكراريّة.** 

الآن، لو نُقارِنُ عمَل الشبكة العصبيّة التكراريّة بعمل الشبكة العصبيّة الترشيحيّة (أو الالتفافيّة)، المشهورة باِسمها المُختصر CNN، نُلاحظ أنّ الأخيرة قادرة على حساب كُلّ المُخرجات بالتوازي اِعتمادً على جُزءٍ من المُدخلات. فهي لا تحتاج إلاّ لمجموعة المُدخلات الموجودة في حيِّز نواة الترشيح حتَّى تقوم بحسابِ المُخرج. وهذا يجعلُ عمليَّة تدريبها أسرع بكثيرٍ من نظيرتها التكراريّة. 

<img class="img-fluid" src="/assets/images/basic-conv-layer.png" alt="توضيح للحساب بالتوازي في نواة ترشيح">

لكن رغم قُدرتها على القيام بالحسابات بالتوازي، فإنّ نُقطَةَ ضعفِ الشبكةِ العصبيّة الترشيحيّة هي محدوديّةُ قُدرَتِها على نمذجةِ الرّوابط بعيدة المدى. ففي طبقةٍ ترشيحيّة واحدة يُمكن للكلمات القريبَةِ من بعضها البعض فقط، بالمُقارنة مع مدى حيِّز عمل النواة، أن تتفاعل فيما بينها. وأمّا إذا أردنا تعلُّم التفاعُلات بعيدة المدى الموجودة في التسلسُل، فيجبُ تكديس عِدَّةِ طبقاتٍ ترشيحية بعضها فوق بعض، وهذا ما يُصعِّبُ اِستعمالها في التعلُّم من سلاسل البيانات طويلة المدى. 

<img class="img-fluid" src="/assets/images/2-layers-convolution.png" alt="رسم توضيحي لشبكة ترشيحية بطبقتين">

ونظرا لنقطة ضعف الشبكة العصبية الترشيحية في التعلُّم التفاعلات بعيدة المدى من سلاسل البيانات وجب تحيينُ هدف التصميم الذي قاد إلى استعمال آلية الاِنتباه في المُحوِّل كتّالي:

**هدفُ التصميم الذّي اِنطُلِقَ منه لبلوغ آلية الاِنتباه هُوَ: اِيجادُ طريقةٍ للتعلُّمِ من سلاسلِ البياناتِ بشكلٍ مُتوازي، وليس تسلسُلي أو تكراري، أي اِيجادُ بديلٍ للتعلُّم من دون الوصلات التكراريّة مع القُدرة على تعلُّم التفاعُلات بعيدة المدى في سلاسل البيانات** 

لذا فهدف تصميم خوارزميّة المُحوِّل بآلية الانتباه كان اِغتنام أفضل خواصِّ الخوارميّات الموجودة حينها دون عيوبها. فهِيَ سعَت إلى نمذجَةِ التّفاعُلات المَوجودة على طُول مدى التسلسُل المُدخل، قريبةً كانت من بعضها أم بعيدة، دون الحاجة إلى اِستعمال الوصلات التكراريّة. ممّا يعني أنّ النموذج الكُلِّي المُعتَمِد على هذه الآلية يُمكن حسابه بالتوازي وبشكل فعّال باِستعمال طريقةِ التغذية الأماميّة. 

### فهرس الألفاظ التقنية ومُرادفاتُها الإنجليزية

- المُحوّل: transformer
- الترميز: encoding
- فكّ الترميز: decoder
- المُرمِّز: encoder
- فاكُّ الترميز: decoder
- دالّة الحدّ الأقصى الليّن: softmax
- طبقة تحويل خطّي: linear transformation layer
- الإنتباه: attention
- الإنتباه مُتعدد الرؤوس: multi-head attention
- التضمين: embedding
- الترميز الموضعي: positional encoding
- التسوية: normalisation
- الإنتباه المُعتمد على الجداء السُلَّمِي المُحجَّم: Scaled Dot-Product Attention
- وصلة البقية: Residual connection

### المصادر
	
1. [الإنتباه هو كُلُّ ما تحتاجه](https://arxiv.org/abs/1706.03762)
2. [التعلُّم من التسلسُل إلى التسلسُل باستعمال الشبكات العصبية](https://arxiv.org/abs/1409.3215)
3. [وصلة البقية](https://paperswithcode.com/method/residual-connection)
4. [التسوية](https://paperswithcode.com/method/batch-normalization) 
